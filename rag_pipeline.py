# -*- coding: utf-8 -*-
"""rag_pipeline

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eSdA8bsdqTLtieB3VuEwPQ4aSA2xMuby
"""

# rag_pipeline.py

from pathlib import Path

from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.llms import HuggingFacePipeline
from langchain.chains import ConversationalRetrievalChain

from transformers import pipeline


BASE_DIR = Path(__file__).resolve().parent
DB_DIR = BASE_DIR / "db"


def get_embeddings():
    """
    Mismo modelo de embeddings que se usa en ingest.py.
    IMPORTANTE que coincida, sino Chroma no va a recuperar bien.
    """
    model_name = "sentence-transformers/all-MiniLM-L6-v2"
    return HuggingFaceEmbeddings(model_name=model_name)


def get_vectorstore():
    """
    Carga la base Chroma existente en ./db
    """
    if not DB_DIR.exists():
        raise FileNotFoundError(
            f"No se encontró la carpeta {DB_DIR}. "
            "Primero corré: python ingest.py"
        )

    embeddings = get_embeddings()
    vectorstore = Chroma(
        persist_directory=str(DB_DIR),
        embedding_function=embeddings,
    )
    return vectorstore


def get_llm():
    """
    Crea un LLM de Hugging Face usando google/flan-t5-base.
    Es más liviano que flan-t5-large y funciona bien para respuestas cortas.
    """
    text2text_pipeline = pipeline(
        "text2text-generation",
        model="google/flan-t5-base",
        max_new_tokens=256,
    )

    llm = HuggingFacePipeline(pipeline=text2text_pipeline)
    return llm


def get_conversational_chain():
    """
    Devuelve una ConversationalRetrievalChain lista para usar.
    """
    llm = get_llm()
    vectorstore = get_vectorstore()

    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

    chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever,
        return_source_documents=True,
        verbose=False,
    )
    return chain


def format_sources(source_documents, max_chars: int = 200):
    """
    Devuelve una lista de dicts con info de las fuentes usadas.
    """
    formatted = []
    for doc in source_documents:
        source = doc.metadata.get("source", "desconocido")
        page = doc.metadata.get("page", "N/A")
        snippet = doc.page_content[:max_chars].replace("\n", " ")
        formatted.append(
            {
                "source": source,
                "page": page,
                "snippet": snippet,
            }
        )
    return formatted